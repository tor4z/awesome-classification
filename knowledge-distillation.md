# Knowledge Distillation

> Do not generalize well when trained on insufficient amounts of data

--[Touvron et al.](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf)

--------------

* [PMLR 2021] Training data-efficient image transformers & distillation through attention [paper](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf) [code](https://github.com/facebookresearch/deit)
