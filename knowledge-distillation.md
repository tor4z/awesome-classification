# Knowledge Distillation

> Do not generalize well when trained on insufficient amounts of data

--[Touvron et al.](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf)

--------------

* [PMLR 2021] Training data-efficient image transformers & distillation through attention [paper](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf) [code](https://github.com/facebookresearch/deit)

* [ICLR 2020] Transferring Inductive Biases through Knowledge Distillation [paper](https://arxiv.org/pdf/2006.00555.pdf) [code](https://github.com/samiraabnar/Reflect)


## Transfer learning

* [NIPS 2020] What is being transferred in transfer learning? [paper](https://arxiv.org/pdf/2008.11687.pdf) [code](https://github.com/google-research/understanding-transfer-learning)
