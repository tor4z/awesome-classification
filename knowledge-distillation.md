# Knowledge Distillation

> Do not generalize well when trained on insufficient amounts of data

--[Touvron et al.](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf)

--------------

* [PMLR 2021] Training data-efficient image transformers & distillation through attention [paper](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf) [code](https://github.com/facebookresearch/deit)

* [ICLR 2020] Transferring Inductive Biases through Knowledge Distillation [paper](https://arxiv.org/pdf/2006.00555.pdf) [code](https://github.com/samiraabnar/Reflect)

* [arXiv 2021] Knowledge distillation: A good teacher is patient and consistent [paper](https://arxiv.org/pdf/2106.05237v1.pdf) [code_jax](https://github.com/google-research/big_transfer) [code_tf2](https://github.com/sayakpaul/FunMatch-Distillation)

* [ICCV 2021] Learning Compatible Embeddings [paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Learning_Compatible_Embeddings_ICCV_2021_paper.pdf) [code](https://github.com/IrvingMeng/LCE)

* [ICCV 2021] Distilling Holistic Knowledge with Graph Neural Networks [paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Distilling_Holistic_Knowledge_With_Graph_Neural_Networks_ICCV_2021_paper.pdf) [code](https://github.com/wyc-ruiker/HKD)

* [arXiv 2021] Multi-granularity for knowledge distillation [paper](https://arxiv.org/pdf/2108.06681v1.pdf) [code](https://github.com/shaoeric/multi-granularity-distillation)

* [ICCV 2021] Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment [paper](http://openaccess.thecvf.com//content/ICCV2021/papers/Zheng_Learning_Conditional_Knowledge_Distillation_for_Degraded-Reference_Image_Quality_Assessment_ICCV_2021_paper.pdf) [code](https://github.com/researchmm/ckdn)

* [ICCV 2021] Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better [paper](http://openaccess.thecvf.com//content/ICCV2021/papers/Zi_Revisiting_Adversarial_Robustness_Distillation_Robust_Soft_Labels_Make_Student_Better_ICCV_2021_paper.pdf) [code](https://github.com/zibojia/rslad)

* [EMNLP 2021] Dynamic Knowledge Distillation for Pre-trained Language Models [paper](https://arxiv.org/pdf/2109.11295v1.pdf) [code](https://github.com/lancopku/DynamicKD)

* [NIPS 2019] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter [paper](https://arxiv.org/pdf/1910.01108v4.pdf) [code1](https://github.com/huggingface/transformers) [code2](https://github.com/huggingface/swift-coreml-transformers)

* [EMNLP 2020] Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation [paper](https://arxiv.org/pdf/2004.09813v2.pdf) [code](https://github.com/UKPLab/sentence-transformers)

* [ICLR 2020] Well-Read Students Learn Better: On the Importance of Pre-training Compact Models [paper](https://arxiv.org/pdf/1908.08962v2.pdf) [code_tf](https://github.com/google-research/bert) [code_torch](https://github.com/PAIR-code/lit)

* [arXiv 2020] Pre-trained Summarization Distillation [paper](https://arxiv.org/pdf/2010.13002v2.pdf) [code1](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md) [code2](http://tiny.cc/4iy0tz)

* [arXiv 2019] Data Efficient Stagewise Knowledge Distillation [paper](https://arxiv.org/pdf/1911.06786v3.pdf) [code](https://github.com/IvLabs/stagewise-knowledge-distillation)

* [CVPR 2020] Revisiting Knowledge Distillation via Label Smoothing Regularization [paper](https://arxiv.org/pdf/1909.11723v3.pdf) [code](https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation)

* [ICLR 2020] Contrastive Representation Distillation [paper](https://arxiv.org/pdf/1910.10699v2.pdf) [code1](https://github.com/HobbitLong/RepDistiller) [code2](https://github.com/yoshitomo-matsubara/torchdistill)

* [ECCV 2020] Knowledge Distillation Meets Self-Supervision [paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562.pdf) [code](https://github.com/xuguodong03/SSKD)

* [arXiv 2020] MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks [paper](https://arxiv.org/pdf/2009.08453v2.pdf) [code](https://github.com/szq0214/MEAL-V2)

* [arXiv 2021] Distilling a Powerful Student Model via Online Knowledge Distillation [paper](https://arxiv.org/pdf/2103.14473v2.pdf) [code](https://github.com/SJLeo/FFSD)


* [CVPR 2019] Relational Knowledge Distillation [paper](https://arxiv.org/pdf/1904.05068.pdf) [code](https://github.com/lenscloth/RKD)

* [ICCV 2019] Similarity-Preserving Knowledge Distillation [paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf)

* [ICCV 2019] Correlation Congruence for Knowledge Distillation [paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.pdf)

* [arXiv 2018] Self-supervised Knowledge Distillation Using Singular Value Decomposition [paper](https://arxiv.org/pdf/1807.06819.pdf) [code](https://github.com/sseung0703/SSKD)

* [ICLR 2016] Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer [paper](https://arxiv.org/pdf/1612.03928.pdf) [code](https://github.com/szagoruyko/attention-transfer)

## Transfer learning

* [NIPS 2020] What is being transferred in transfer learning? [paper](https://arxiv.org/pdf/2008.11687.pdf) [code](https://github.com/google-research/understanding-transfer-learning)
