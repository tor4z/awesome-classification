# Knowledge Distillation

> Do not generalize well when trained on insufficient amounts of data

--[Touvron et al.](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf)

--------------

* [PMLR 2021] Training data-efficient image transformers & distillation through attention [paper](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf) [code](https://github.com/facebookresearch/deit)

* [ICLR 2020] Transferring Inductive Biases through Knowledge Distillation [paper](https://arxiv.org/pdf/2006.00555.pdf)
