# Awesome vision transformer

* [ICLR 2021] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [paper](https://arxiv.org/pdf/2010.11929.pdf) [code](https://github.com/google-research/vision_transformer)

* [NIPS 2021] Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning [paper](https://arxiv.org/pdf/2110.05340v1.pdf) [code](https://github.com/ChongjianGE/CARE)

* [ICLR 2020] BEIT: BERT Pre-Training of Image Transformers [paper](https://arxiv.org/pdf/2106.08254v1.pdf) [code](https://github.com/microsoft/unilm/tree/master/beit)

* [PAMI 2021] P2T: Pyramid Pooling Transformer for Scene Understanding [paper](https://arxiv.org/pdf/2106.12011v3.pdf) [code](https://github.com/yuhuan-wu/P2T)

* [arXiv 2021] VOLO: Vision Outlooker for Visual Recognition [paper](https://arxiv.org/pdf/2106.13112v2.pdf) [code](https://github.com/sail-sg/volo)

* [ICCV 2021] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions [paper](https://arxiv.org/pdf/2102.12122.pdf) [code](https://github.com/whai362/PVT)

* [arXiv 2021] PVTv2: Improved Baselines with Pyramid Vision Transformer [paper](https://arxiv.org/pdf/2106.13797v4.pdf) [code](https://github.com/whai362/PVT)

* [NIPS 2021] Focal Self-attention for Local-Global Interactions in Vision Transformers [paper](https://arxiv.org/pdf/2107.00641v1.pdf) [code](https://github.com/microsoft/Focal-Transformer)

* [ICCV 2021] AutoFormer: Searching Transformers for Visual Recognition [paper](https://arxiv.org/pdf/2107.00651v1.pdf) [code](https://github.com/microsoft/Cream)

* [ICCV 2021] GLiT: Neural Architecture Search for Global and Local Image Transformer [paper](https://arxiv.org/pdf/2107.02960v3.pdf) [code](https://github.com/bychen515/GLiT)

* [arXiv 2021] Visual Parser: Representing Part-whole Hierarchies with Transformers [paper](https://arxiv.org/pdf/2107.05790v1.pdf) [code](https://github.com/kevin-ssy/ViP)

* [arXiv 2021]  CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention [paper](https://arxiv.org/pdf/2108.00154v2.pdf) [code](https://github.com/cheerss/CrossFormer)

* [arXiv 2021] Training data-efficient image transformers & distillation through attention [paper](https://arxiv.org/pdf/2012.12877.pdf) [code](https://github.com/facebookresearch/deit)

* [ICCV 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows [paper](https://arxiv.org/pdf/2103.14030.pdf) [code](https://github.com/microsoft/Swin-Transformer)

## Position encoding

* [ACL 2018] Self-Attention with Relative Position Representations [paper](https://arxiv.org/pdf/1803.02155.pdf)

* [ACL 2019] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [paper](https://aclanthology.org/P19-1285.pdf) [code](https://github.com/kimiyoung/transformer-xl)

* [ICCV 2021] Rethinking and Improving Relative Position Encoding for Vision Transformer [paper](https://arxiv.org/pdf/2107.14222v1.pdf) [code](https://github.com/microsoft/Cream/tree/main/iRPE)

* [EMNLP 2020] Improve Transformer Models with Better Relative Position Embeddings [paper](https://arxiv.org/pdf/2009.13658.pdf)

## Attention

* [NIPS 2019] Stand-Alone Self-Attention in Vision Models [paper](https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf) [code](https://github.com/google-research/google-research/tree/master/standalone_self_attention_in_vision_models) [code-thirdparty](https://github.com/leaderj1001/Stand-Alone-Self-Attention)

* [ICCV 2019] Attention Augmented Convolutional Networks [paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.pdf)

* [CVPR 2020] Exploring Self-attention for Image Recognition [paper](https://jiaya.me/papers/selfatten_cvpr20.pdf)

* [ECCV 2020] Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation [paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490103.pdf)

## Sampling

* [ICCV 2021] Vision Transformer with Progressive Sampling [paper](https://arxiv.org/pdf/2108.01684.pdf) [code](https://github.com/yuexy/PS-ViT)

## Non-attention

* [arXiv 2021] MLP-Mixer: An all-MLP Architecture for Vision [paper](https://arxiv.org/pdf/2105.01601.pdf) [code](https://github.com/google-research/vision_transformer)

* [NIPS 2021] Global Filter Networks for Image Classification [paper](https://arxiv.org/pdf/2107.00645v1.pdf) [code](https://github.com/raoyongming/GFNet)

* [arXiv 2021] FNet: Mixing Tokens with Fourier Transforms [paper](https://arxiv.org/pdf/2105.03824.pdf) [code](https://github.com/google-research/google-research/) [code-thirdparty](https://github.com/rishikksh20/FNet-pytorch)

## Train a ViT

* [arXiv 2021] How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers [paper](https://arxiv.org/pdf/2106.10270.pdf) [code](https://github.com/google-research/vision_transformer)

## ViT vs. CNNs

* [arXiv 2021] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations [paper](https://arxiv.org/pdf/2106.01548.pdf) [code](https://github.com/google-research/vision_transformer)
