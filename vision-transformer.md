# Awesome vision transformer

* [ICLR 2021] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [paper](https://arxiv.org/pdf/2010.11929.pdf) [code](https://github.com/google-research/vision_transformer)

* [NIPS 2021] Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning [paper](https://arxiv.org/pdf/2110.05340v1.pdf) [code](https://github.com/ChongjianGE/CARE)

* [ICLR 2020] BEIT: BERT Pre-Training of Image Transformers [paper](https://arxiv.org/pdf/2106.08254v1.pdf) [code](https://github.com/microsoft/unilm/tree/master/beit)

* [PAMI 2021] P2T: Pyramid Pooling Transformer for Scene Understanding [paper](https://arxiv.org/pdf/2106.12011v3.pdf) [code](https://github.com/yuhuan-wu/P2T)

* [arXiv 2021] VOLO: Vision Outlooker for Visual Recognition [paper](https://arxiv.org/pdf/2106.13112v2.pdf) [code](https://github.com/sail-sg/volo)

* [ICCV 2021] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions [paper](https://arxiv.org/pdf/2102.12122.pdf) [code](https://github.com/whai362/PVT)

* [arXiv 2021] PVTv2: Improved Baselines with Pyramid Vision Transformer [paper](https://arxiv.org/pdf/2106.13797v4.pdf) [code](https://github.com/whai362/PVT)

* [NIPS 2021] Focal Self-attention for Local-Global Interactions in Vision Transformers [paper](https://arxiv.org/pdf/2107.00641v1.pdf) [code](https://github.com/microsoft/Focal-Transformer)

* [ICCV 2021] AutoFormer: Searching Transformers for Visual Recognition [paper](https://arxiv.org/pdf/2107.00651v1.pdf) [code](https://github.com/microsoft/Cream)

* [ICCV 2021] GLiT: Neural Architecture Search for Global and Local Image Transformer [paper](https://arxiv.org/pdf/2107.02960v3.pdf) [code](https://github.com/bychen515/GLiT)

* [arXiv 2021] Visual Parser: Representing Part-whole Hierarchies with Transformers [paper](https://arxiv.org/pdf/2107.05790v1.pdf) [code](https://github.com/kevin-ssy/ViP)

* [arXiv 2021]  CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention [paper](https://arxiv.org/pdf/2108.00154v2.pdf) [code](https://github.com/cheerss/CrossFormer)

* [arXiv 2021] Training data-efficient image transformers & distillation through attention [paper](https://arxiv.org/pdf/2012.12877.pdf) [code](https://github.com/facebookresearch/deit)

* [ICCV 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows [paper](https://arxiv.org/pdf/2103.14030.pdf) [code](https://github.com/microsoft/Swin-Transformer)

* [ICLR 2021] Deformable DETR: Deformable Transformers for End-to-End Object Detection [paper](https://openreview.net/pdf?id=gZ9hCDWe6ke) [code](https://github.com/fundamentalvision/Deformable-DETR)

## Position encoding

* [ACL 2018] Self-Attention with Relative Position Representations [paper](https://arxiv.org/pdf/1803.02155.pdf)

* [ACL 2019] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [paper](https://aclanthology.org/P19-1285.pdf) [code](https://github.com/kimiyoung/transformer-xl)

* [ICCV 2021] Rethinking and Improving Relative Position Encoding for Vision Transformer [paper](https://arxiv.org/pdf/2107.14222v1.pdf) [code](https://github.com/microsoft/Cream/tree/main/iRPE)

* [EMNLP 2020] Improve Transformer Models with Better Relative Position Embeddings [paper](https://arxiv.org/pdf/2009.13658.pdf)

* [arXiv 2021] RoFormer: Enhanced Transformer with Rotary Position Embedding [paper](https://arxiv.org/pdf/2104.09864.pdf) [code](https://github.com/ZhuiyiTechnology/roformer)

## Attention

* [NIPS 2019] Stand-Alone Self-Attention in Vision Models [paper](https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf) [code](https://github.com/google-research/google-research/tree/master/standalone_self_attention_in_vision_models) [code-thirdparty](https://github.com/leaderj1001/Stand-Alone-Self-Attention)

* [ICCV 2019] Attention Augmented Convolutional Networks [paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.pdf)

* [CVPR 2020] Exploring Self-attention for Image Recognition [paper](https://jiaya.me/papers/selfatten_cvpr20.pdf)

* [ECCV 2020] Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation [paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490103.pdf)

## Sampling

* [ICCV 2021] Vision Transformer with Progressive Sampling [paper](https://arxiv.org/pdf/2108.01684.pdf) [code](https://github.com/yuexy/PS-ViT)

## Non-attention

* [arXiv 2021] MLP-Mixer: An all-MLP Architecture for Vision [paper](https://arxiv.org/pdf/2105.01601.pdf) [code](https://github.com/google-research/vision_transformer)

* [NIPS 2021] Global Filter Networks for Image Classification [paper](https://arxiv.org/pdf/2107.00645v1.pdf) [code](https://github.com/raoyongming/GFNet)

* [arXiv 2021] FNet: Mixing Tokens with Fourier Transforms [paper](https://arxiv.org/pdf/2105.03824.pdf) [code](https://github.com/google-research/google-research/) [code-thirdparty](https://github.com/rishikksh20/FNet-pytorch)

* [ICLR 2021] LambdaNetworks: Modeling long-range Interactions without Attention [paper](https://openreview.net/pdf?id=xTJEN-ggl1b) [code](https://github.com/leaderj1001/LambdaNetworks)

## Linear transformers

* [arXiv 2021] Vision Xformers: Efficient Attention for Image Classification [paper](https://arxiv.org/pdf/2107.02239v4.pdf) [code](https://github.com/pranavphoenix/VisionXformer)

* [AAAI 2021] Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention [paper](https://arxiv.org/pdf/2102.03902.pdf) [code](https://github.com/mlpen/Nystromformer)

* [ICML 2020]  Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention [paper](https://arxiv.org/pdf/2006.16236.pdf) [code](https://github.com/idiap/fast-transformers)

* [ICLR 2021] Rethinking Attention with Performers [paper](https://arxiv.org/pdf/2009.14794.pdf) [code](https://github.com/google-research/google-research/tree/master/performer)

* [arXiv 2020] Linformer: Self-Attention with Linear Complexity [code](https://arxiv.org/pdf/2006.04768.pdf) [code](https://github.com/tatp22/linformer-pytorch)

## Inductive Biases [wiki](https://en.wikipedia.org/wiki/Inductive_bias)

* [arXiv 2021] Inductive Biases for Deep Learning of Higher-Level Cognition [paper](https://arxiv.org/pdf/2011.15091.pdf)

* [ICLR 2021] What they do when in doubt: a study of inductive biases in seq2seq learners [paper](https://openreview.net/pdf?id=YmA86Zo-P_t)

* [ICML 2021] Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers [paper](https://arxiv.org/pdf/2106.13122.pdf) [code](https://github.com/katelyn98/CorruptionRobustness)

* [NIPS 2021] ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias [paper](https://arxiv.org/pdf/2106.03348) [code](https://github.com/Annbless/ViTAE)

* [ICML 2021] ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases [paper](http://proceedings.mlr.press/v139/d-ascoli21a/d-ascoli21a.pdf) [code](https://github.com/facebookresearch/convit)

## Train a ViT

* [arXiv 2021] How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers [paper](https://arxiv.org/pdf/2106.10270.pdf) [code](https://github.com/google-research/vision_transformer)

## ViT vs. CNNs

* [arXiv 2021] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations [paper](https://arxiv.org/pdf/2106.01548.pdf) [code](https://github.com/google-research/vision_transformer)
